{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8800c175",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91fe863f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.1.3\n",
      "  Downloading scikit_learn-1.1.3-cp39-cp39-win_amd64.whl (7.6 MB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in e:\\test_move1\\lib\\site-packages (from scikit-learn==1.1.3) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in e:\\test_move1\\lib\\site-packages (from scikit-learn==1.1.3) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in e:\\test_move1\\lib\\site-packages (from scikit-learn==1.1.3) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in e:\\test_move1\\lib\\site-packages (from scikit-learn==1.1.3) (1.7.3)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "Successfully installed scikit-learn-1.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f14fbd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_subplots\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlxtend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SequentialFeatureSelector \u001b[38;5;28;01mas\u001b[39;00m SFS\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#read .csv data into pd \u001b[39;00m\n\u001b[0;32m     13\u001b[0m data_movie \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_0.csv\u001b[39m\u001b[38;5;124m'\u001b[39m);\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "#read .csv data into pd \n",
    "data_movie = pd.read_csv('dataset_0.csv');\n",
    "data_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_movie.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76aeae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the missing values\n",
    "data_movie =data_movie .replace(r'^\\s*$',np.nan, regex=True)\n",
    "data_movie.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec49d820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Delete the missing values\n",
    "data_movie.dropna(inplace=True)\n",
    "data_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the missing values\n",
    "data_movie =data_movie .replace(r'^\\s*$',np.nan, regex=True)\n",
    "data_movie.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ffc67a",
   "metadata": {},
   "source": [
    "# Filter for action movie and comedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac830bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter action movie and comedy\n",
    "action = data_movie[data_movie['genre'].isin(['Action'])]\n",
    "comedy = data_movie[data_movie['genre'].isin(['Comedy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e834080",
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8442bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "action['movie'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy['movie'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab816f",
   "metadata": {},
   "source": [
    "## Q1.1 Topic modelling of action movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce7d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#aggerate all the tweets into one file and generate the word cloud\n",
    "review_action = action['review']\n",
    "all_review = ''.join(review_action.tolist())\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "wordcloud = WordCloud(background_color=\"white\", colormap='tab10', max_words=200).generate(all_review)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7af06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the text to lower case\n",
    "review_action = [text.lower() for text in review_action]\n",
    "\n",
    "#print the first 3 tweets\n",
    "print(review_action[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d3e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct lemmatization for the words in the text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokens=[]\n",
    "for sent in review_action:\n",
    "    temp=[WordNetLemmatizer().lemmatize(word) for word in sent.split(\" \")]\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a1011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized the stopwords\n",
    "from sklearn.feature_extraction import text \n",
    "my_additional_stop_words = [\"movie\",\"just\",'film',\"one\",'action','plot','character','scene']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14995d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Set how many topics we need\n",
    "NUM_TOPICS = 10\n",
    "action['tokens']=tokens\n",
    "text_train = list(action['tokens'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts.\n",
    "## min_df: ignore terms that have a document frequency strictly lower than the given threshold\n",
    "## max_df: ignore terms that have a document frequency strictly higher than the given threshold\n",
    "## stop_words: ‘english’, list\n",
    "## lowercase: Convert all characters to lowercase before tokenizing.\n",
    "## token_pattern: Regular expression denoting what constitutes a “token”\n",
    "vectoriser = CountVectorizer(min_df=5, max_df=0.9,\n",
    "                             stop_words=stop_words,lowercase=True,\n",
    "                             token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "\n",
    "data_vectorized= vectoriser.fit_transform(text_train)\n",
    "\n",
    "# Build a Latent Dirichlet Allocation Model\n",
    "lda_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, random_state=258, learning_method='online')\n",
    "lda_Z = lda_model.fit_transform(data_vectorized)\n",
    " \n",
    "#text = \"mclaughlin: we can win front row qualifying only the start for volvo says young gun\"\n",
    "#x = lda_model.transform(vectorizer.transform([text]))[0]\n",
    "#print(x, x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c78e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top 10 most frequent words in each topic¶\n",
    "\n",
    "def print_topics(model, vectoriser, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx+1))\n",
    "        print([(vectoriser.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    " \n",
    "print(\"LDA Model:\")\n",
    "print_topics(lda_model, vectoriser)\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9160f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lda_model.transform(data_vectorized)\n",
    "print(x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c095382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics=pd.DataFrame(x)\n",
    "topics.columns=['topic1','topic2','topic3','topic4','topic5','topic6','topic7','topic8','topic9','topic10']\n",
    "\n",
    "action = action.reset_index(drop=True)\n",
    "\n",
    "topics['review'] = action['review']\n",
    "topics['review_post_date'] = action['review_post_date']\n",
    "\n",
    "\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e143724",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.lda_model\n",
    " \n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# The parameters wee need,\n",
    "## LDA model: lda_model\n",
    "## vectorized model: data_vectorized\n",
    "## matrix of token counts: vectoriser\n",
    "\n",
    "panel = pyLDAvis.lda_model.prepare(lda_model, data_vectorized, vectoriser, sort_topics = False)\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef8449e",
   "metadata": {},
   "source": [
    "## Q2.1 Sentiment analysis for action movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d8c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd95821",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# import `SentimentIntensityAnalyzer` and load a model\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentiment = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c198c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model to process each tweet and call `compound` as polarity score\n",
    "\n",
    "scores=[]\n",
    "for tex in topics['review']:\n",
    "    sentimentResults = sentiment.polarity_scores(tex)\n",
    "    score = sentimentResults[\"compound\"]\n",
    "    scores.append(score)\n",
    "\n",
    "# Show the score of index 1 \n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sentiment scores weighted by the topic relevance probability\n",
    "### Create a new variable named 'topic_senti', and the values are sentiment score * topic relevance probability\n",
    "\n",
    "topics['topic1_senti'] = topics['topic1'] * scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 2\n",
    "topics['topic5_senti'] = topics['topic5'] * scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sentiment score for the 2 largest topics for each event day\n",
    "\n",
    "topic_movie_data = topics.groupby([pd.Grouper('review_post_date')]).agg(topic1_sentiment=('topic1_senti', 'mean'),\n",
    "                                                                              topic5_sentiment=('topic5_senti','mean'))\n",
    "topic_movie_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b970428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of each column\n",
    "average_sentiment = topic_movie_data.mean()\n",
    "\n",
    "print(average_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e1d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall sentiment scores\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "text_tweets = action['review'].to_string(index=False)  \n",
    "sentiment_scores=sentiment.polarity_scores(text_tweets)\n",
    "sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2687b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(topic_movie_data, action, left_index=True, right_on='review_post_date')\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006560f4",
   "metadata": {},
   "source": [
    "## Q3.1 Regression models of action movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6820ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=merged_data['box_office_revenue']\n",
    "\n",
    "# change y to a very small number if it's 0, because denominator cannot be 0\n",
    "y[y==0]=0.0001\n",
    "\n",
    "X = merged_data[['num_helpful','num_response','budget','max_screens','topic1_sentiment','topic5_sentiment']]\n",
    "y = np.log(y)\n",
    "\n",
    "X2 = sm.add_constant(X)\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def forward_stepwise(X, y, threshold_in):\n",
    "    initial_features = X.columns.tolist()\n",
    "    best_features = []\n",
    "    \n",
    "    while len(initial_features) > 0:\n",
    "        remaining_features = list(set(initial_features) - set(best_features))\n",
    "        new_pval = pd.Series(index=remaining_features)\n",
    "        \n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[best_features + [new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "            \n",
    "        min_p_value = new_pval.min()\n",
    "        if min_p_value < threshold_in:\n",
    "            best_features.append(new_pval.idxmin())\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return best_features\n",
    "\n",
    "# Use the function to get the best features\n",
    "best_features = forward_stepwise(X, y, 0.05)\n",
    "\n",
    "print(best_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab873f76",
   "metadata": {},
   "source": [
    "### Gradient Boosting Continuous DV for action  revenue vs other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e3e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Set the variables\n",
    "x= merged_data[['max_screens', 'topic5_sentiment', 'budget', 'topic1_sentiment', 'num_response']]\n",
    "\n",
    "y = merged_data['box_office_revenue']\n",
    "y[y==0]=0.0001\n",
    "y = np.log(y)\n",
    "\n",
    "# spilt the training and testing set by 75% and 25% separately\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e742db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load and fit the model\n",
    "import xgboost as xgb\n",
    "xgb_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3,        \n",
    "                  learning_rate = 0.1, max_depth = 5, alpha = 10, n_estimators = 200)\n",
    "xgb_reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de0585",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_reg.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414eb39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg_visual = pd.DataFrame(list(xgb_reg.feature_importances_), \n",
    "              columns =['Action Movie Feature Importance'], index=[ 'max_screens', 'topic5_sentiment', 'budget', 'topic1_sentiment', 'num_response']) \n",
    "\n",
    "xgb_reg_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a5644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the 'f_importances' function to visualize feature importances\n",
    "def f_importances(importance, names):\n",
    "    sorted_importance = importance.argsort()[::-1]\n",
    "    sorted_names = [names[i] for i in sorted_importance]\n",
    "    \n",
    "    # Create a bar plot\n",
    "    plt.barh(range(len(names)), importance[sorted_importance], align='center', color=(0.2, 0.4, 0.6, 0.6))\n",
    "    # Add feature names as labels to x-axis\n",
    "    plt.yticks(range(len(names)), sorted_names)\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('XGBoost: Continuous DV Model for action movies revenue vs other features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620de282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances and feature names\n",
    "importance = xgb_reg.feature_importances_\n",
    "names = x.columns\n",
    "f_importances(importance, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931e87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error rate\n",
    "error_rate = 1 - xgb_reg.score(x_test, y_test)\n",
    "print(\"Error rate: \", error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff735cd5",
   "metadata": {},
   "source": [
    "## Q1.2 Topic modelling of comedy movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6f2b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Wordcloud\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#aggerate all the tweets into one file and generate the word cloud\n",
    "review_comedy = comedy['review']\n",
    "all_review = ''.join(review_comedy.tolist())\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "wordcloud = WordCloud(background_color=\"white\", colormap='tab10', max_words=200).generate(all_review)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the text to lower case\n",
    "review_comedy = [text.lower() for text in review_comedy]\n",
    "\n",
    "#print the first 3 tweets\n",
    "print(review_comedy[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31978b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct lemmatization for the words in the text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokens=[]\n",
    "for sent in review_comedy:\n",
    "    temp=[WordNetLemmatizer().lemmatize(word) for word in sent.split(\" \")]\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized the stopwords\n",
    "from sklearn.feature_extraction import text \n",
    "my_additional_stop_words = ['movie', 'one', 'film','just','really','comedy','seen','people','like','funny','make','lot','way','think','movies']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c277c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3190b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# import `SentimentIntensityAnalyzer` and load a model\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentiment = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Set how many topics we need\n",
    "NUM_TOPICS = 10\n",
    "comedy['tokens']=tokens\n",
    "text_train = list(comedy['tokens'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts.\n",
    "## min_df: ignore terms that have a document frequency strictly lower than the given threshold\n",
    "## max_df: ignore terms that have a document frequency strictly higher than the given threshold\n",
    "## stop_words: ‘english’, list\n",
    "## lowercase: Convert all characters to lowercase before tokenizing.\n",
    "## token_pattern: Regular expression denoting what constitutes a “token”\n",
    "vectoriser = CountVectorizer(min_df=5, max_df=0.9,\n",
    "                             stop_words=stop_words,lowercase=True,\n",
    "                             token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "\n",
    "data_vectorized= vectoriser.fit_transform(text_train)\n",
    "\n",
    "# Build a Latent Dirichlet Allocation Model\n",
    "lda_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, random_state=258, learning_method='online')\n",
    "lda_Z = lda_model.fit_transform(data_vectorized)\n",
    " \n",
    "#text = \"mclaughlin: we can win front row qualifying only the start for volvo says young gun\"\n",
    "#x = lda_model.transform(vectorizer.transform([text]))[0]\n",
    "#print(x, x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a8e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4292f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top 10 most frequent words in each topic¶\n",
    "\n",
    "def print_topics(model, vectoriser, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx+1))\n",
    "        print([(vectoriser.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    " \n",
    "print(\"LDA Model:\")\n",
    "print_topics(lda_model, vectoriser)\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e08d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lda_model.transform(data_vectorized)\n",
    "print(x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcabf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics=pd.DataFrame(x)\n",
    "topics.columns=['topic1','topic2','topic3','topic4','topic5','topic6','topic7','topic8','topic9','topic10']\n",
    "\n",
    "comedy = comedy.reset_index(drop=True)\n",
    "\n",
    "topics['review'] = comedy['review']\n",
    "topics['review_post_date'] = comedy['review_post_date']\n",
    "\n",
    "\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46baca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.lda_model\n",
    " \n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# The parameters wee need,\n",
    "## LDA model: lda_model\n",
    "## vectorized model: data_vectorized\n",
    "## matrix of token counts: vectoriser\n",
    "\n",
    "panel = pyLDAvis.lda_model.prepare(lda_model, data_vectorized, vectoriser, sort_topics = False)\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47587e9e",
   "metadata": {},
   "source": [
    "## Q2.2 Sentiment analysis for comedy movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144313a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce385a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# import `SentimentIntensityAnalyzer` and load a model\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentiment = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c6d00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using the model to process each tweet and call `compound` as polarity score\n",
    "\n",
    "scores=[]\n",
    "for tex in topics['review']:\n",
    "    sentimentResults = sentiment.polarity_scores(tex)\n",
    "    score = sentimentResults[\"compound\"]\n",
    "    scores.append(score)\n",
    "\n",
    "# Show the score of index 1 \n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25920136",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3643223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sentiment scores weighted by the topic relevance probability\n",
    "### Create a new variable named 'topic_senti', and the values are sentiment score * topic relevance probability\n",
    "\n",
    "topics['topic2_senti'] = topics['topic2'] * scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a06e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 2\n",
    "topics['topic5_senti'] = topics['topic5'] * scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befcfec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sentiment score for the 2 largest topics for each event day\n",
    "\n",
    "topic_movie_data = topics.groupby([pd.Grouper('review_post_date')]).agg(topic2_sentiment=('topic2_senti', 'mean'),\n",
    "                                                                              topic5_sentiment=('topic5_senti','mean'))\n",
    "topic_movie_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of each column\n",
    "average_sentiment = topic_movie_data.mean()\n",
    "\n",
    "print(average_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall sentiment scores\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "text_tweets = comedy['review'].to_string(index=False)  \n",
    "sentiment_scores=sentiment.polarity_scores(text_tweets)\n",
    "sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9741d52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merged_data = pd.merge(topic_movie_data, comedy, left_index=True, right_on='review_post_date')\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb5a677",
   "metadata": {},
   "source": [
    "## Q3.2 Regression models of comedy movies¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685eaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=merged_data['box_office_revenue']\n",
    "\n",
    "# change y to a very small number if it's 0, because denominator cannot be 0\n",
    "y[y==0]=0.0001\n",
    "\n",
    "X = merged_data[['num_helpful','num_response','budget','max_screens','topic2_sentiment','topic5_sentiment']]\n",
    "y = np.log(y)\n",
    "\n",
    "X2 = sm.add_constant(X)\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1067d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def forward_stepwise(X, y, threshold_in):\n",
    "    initial_features = X.columns.tolist()\n",
    "    best_features = []\n",
    "    \n",
    "    while len(initial_features) > 0:\n",
    "        remaining_features = list(set(initial_features) - set(best_features))\n",
    "        new_pval = pd.Series(index=remaining_features)\n",
    "        \n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[best_features + [new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "            \n",
    "        min_p_value = new_pval.min()\n",
    "        if min_p_value < threshold_in:\n",
    "            best_features.append(new_pval.idxmin())\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return best_features\n",
    "\n",
    "# Use the function to get the best features\n",
    "best_features = forward_stepwise(X, y, 0.05)\n",
    "\n",
    "print(best_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f70b2a6",
   "metadata": {},
   "source": [
    "### Continuous random forest model for comendy revenue vs other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Set the variables\n",
    "x= merged_data[['max_screens', 'budget', 'topic2_sentiment', 'topic5_sentiment']]\n",
    "\n",
    "y = merged_data['box_office_revenue']\n",
    "y[y==0]=0.0001\n",
    "y = np.log(y)\n",
    "\n",
    "# spilt the training and testing set by 75% and 25% separately\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rnd_reg = RandomForestRegressor(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_reg.fit(x_train, y_train)\n",
    "y_pred = rnd_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa70449",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c4532",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e280f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_comparison=pd.DataFrame({'y_test': y_test,\n",
    "                           'y_pred': y_pred})\n",
    "pd.DataFrame(y_comparison).to_csv('y_comparison.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean accuracy using the score method\n",
    "error_rate = 1.4-rnd_reg.score(x_test.values, y_test.values)\n",
    "print(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd35f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate RMSE\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "MSE = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b946700",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rnd_visual = pd.DataFrame(list(rnd_reg.feature_importances_), columns=['Feature Importance'], index=['max_screens', 'budget', 'topic2_sentiment', 'topic5_sentiment'])\n",
    "df_rnd_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9747e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# A function to visualise the feature importance or coef\n",
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center',color = (0.2, 0.4, 0.6, 0.6))\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.xlabel('Coefficient rating',fontsize=12)\n",
    "    plt.ylabel('Features',fontsize=12)\n",
    "    plt.title(\"Continuous random forest model for comendy revenue vs other features\",fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "features_names = ['max_screens', 'budget', 'topic2_sentiment', 'topic5_sentiment']\n",
    "rnd_reg = RandomForestRegressor(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_reg.fit(x_train, y_train)\n",
    "y_pred = rnd_reg.predict(x_test)\n",
    "\n",
    "rnd_importance = rnd_reg.feature_importances_\n",
    "f_importances(rnd_importance, features_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
